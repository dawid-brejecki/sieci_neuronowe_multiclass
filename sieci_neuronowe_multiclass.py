# -*- coding: utf-8 -*-
"""sieci_neuronowe_multiclass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pMC7dGtczoKzCpFIMtcLthgU1oyOYtEW
"""

import numpy as np
import pandas as pd
from keras.utils.np_utils import to_categorical 
from keras import models
from keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from keras.utils.vis_utils import model_to_dot
from IPython.display import SVG
import matplotlib.pyplot as plt
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras import regularizers

# preprocessing
df=pd.read_csv('bodyPerformance.csv')
df = df.dropna()
le = LabelEncoder()
df['gender'] = le.fit_transform(df['gender'])
df['class'] = le.fit_transform(df['class'])

column_names_to_not_normalize = ['gender', 'class']
column_names_to_normalize = [x for x in list(df) if x not in column_names_to_not_normalize]

scaler = StandardScaler()
df[column_names_to_normalize] = scaler.fit_transform(df[column_names_to_normalize])

target = df.pop('class')
target=to_categorical(target)
X_train, X_test, Y_train, Y_test = train_test_split(df, target, test_size = 0.25)

# budowanie modelu
network = models.Sequential()
# Dodanie warstwy wejsciowej z funkcją aktywacji ReLU
network.add(layers.Dense(units=11, activation='relu', input_shape=(11,)))
# zapobieganie przeuczeniu
network.add(layers.Dropout(0.2))
# Dodanie warstwy ukrytej z funkcją aktywacji ReLU.
network.add(layers.Dense(units=300, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
# zapobieganie przeuczeniu
network.add(layers.Dropout(0.5))
# Dodanie warstwy wyjsciowej z funkcją aktywacji softmax - typowej dla multiclass.
network.add(layers.Dense(units=4, activation='softmax'))
# Kompilacja sieci neuronowej. 
network.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
# Zakonczenie procesu uczenia w momencie zwiekszajacej sie straty zbioru testowego i zapis najlepszego modelu
callbacks = [EarlyStopping(monitor="val_loss", patience=1), ModelCheckpoint(filepath="best_model.h5",
monitor="val_loss", save_best_only=True)]
# Wytrenowanie sieci neuronowej.
history = network.fit(X_train, # Cechy.
                      Y_train, # Wektor docelowy.
                      epochs=100,
                      verbose=1, # opis
                      validation_data=(X_test, Y_test))

# predykcja
Y_pred = network.predict(X_test)
# wizualizacja funkcji straty
training_loss = history.history["loss"] 
test_loss = history.history["val_loss"]

epoch_count = range(1, len(training_loss) + 1)

plt.plot(epoch_count, training_loss, "r--")
plt.plot(epoch_count, test_loss, "b-")
plt.legend(["Strata zbioru uczącego", "Strata zbioru testowego"]) 
plt.xlabel("Epoka")
plt.ylabel("Strata")
plt.show()

# z wykresu wynika, że optymalna bylaby liczba ok 45 epok

# wizualizacja dokladnosci
training_accuracy = history.history["accuracy"] 
test_accuracy = history.history["val_accuracy"] 
plt.plot(epoch_count, training_accuracy, "r--") 
plt.plot(epoch_count, test_accuracy, "b-")

plt.legend(["Dokładność zbioru uczącego", "Dokładność zbioru testowego"]) 
plt.xlabel("Epoka")
plt.ylabel("Wynik dokładności")
plt.show();

# wyswietlenie wykresu architektury sieci
SVG(model_to_dot(network, show_shapes=True).create(prog="dot", format="svg"))

Y_pred = network.predict(X_test)

